%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Experimental Setup:
%%  - What our methodology was
%%  - Details about the Phones
%%  - Some information about NIST
%%
%%%%% Replacing these as well:
%%% \section{Plan for Development}
%%% \input{tasks}
%%% 
%%% \subsection{Risks and Unknowns}
%%% \input{risks_and_unknowns}
%%% 
%%% \section{Deliverables}
%%% \input{deliverables}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In order to show that sampling accelerometer and gyroscope data is an effective
mechanism for randomness generation on mobile phones, a testing methodology was
developed. This testing methodology involved gathering data from mobile phones
and utilizing well regarded statistical analysis suites to determine the quality
of the results. To maintain data equivalence across testing, a single platform
was chosen for all of the tests. 

%%% Information about the Phone
%%%   - Specs
%%%   - Why it was chosen over other phones... lol
\subsection{Samsung Nexus S}

The platform used for testing was the Samsung Nexus S. The Nexus S was first
released in December of 2010. It uses a Samsung Exynos 3110 System on Chip
(SoC). The Exynos 3110 is a single core ARM Cortex-A8 using the ARMv7
instruction set. In the Nexus S, the processor operates at a 1GHz clock
frequency and is paired with 512MB of main memory. The phone originally shipped
with Android 2.3 Gingerbread installed but was updated to the latest supported
build, 4.1.2 Jelly Bean.  The Nexus S has both a three-axis gyroscope and
accelerometer provided by the InvenSense MPU-6050. The MPU-6050 communicates to
the CPU via an I\textsuperscript{2}C bus operating at 400KHz. 

The Nexus S was selected because it represented both a vanilla Android mobile
phone, but also because it is an example of a widely popular mobile phone that
does not have a hardware random number generator. As such it was a good first
target for implementing an additional source of entropy for the kernel's entropy
pool. Ideally, by providing fast random number generation on a platform that
suffers from a reduced number of sources of entropy, improved cryptographic
speed as well as security could be brought to users of such devices.


%%% Background about NIST
\subsection{NIST Statistical Testing Suite}

The National Institute of Standards and Technology (NIST) publishes a
statistical testing suite for the verification of random number generators.
Described in Special Publication 800-22 \cite{nist}, the \textit{Statistical
Test Suite for Random and Pseudorandom Number Generators for Cryptographic
Applications} comprises 15 statistical tests that analyze a source of bitstreams
for the apparent lack of entropy. The documentation does note both that some
tests are expected to fail even for good pseudorandom number generators and it
should further be noted that even a random number generator that appears to pass
these tests is not necessarily random, it just has the appearance thereof.

The NIST statistical test suite was selected for use because it provides a
rigorous testing suite that is recognized as providing quality statistical
analysis of apparently random bitstreams. This statistical test suite was used
to test the quality of potentially random data from several sources, including
unprocessed data from the sensors, random devices from the stock system, as well
as several modifications on adding data from the sensors to the kernel's entropy
pool and drawing from the randomness devices.

% NIST-> How does it work and how do we get it.
NIST takes a binary input file and allows the user to define the length of the
bitstream as well the number of bitstreams pulled from the file. After that is
defined, the software will drive several statistical tests against the data.
Each test defined what it means to ``pass'' the test for a given bitstream. The
NIST statistical test suite then returns the proportion of tests that passed.
For the testing done in this work, 10 bitstreams were used. For a 10 bitstream
test, a passing random number generator is expected to have at least a 9 of the
10 tests passing.

%%% What was developed in order to perform the testing?
\subsection{Data Collection Software}

In order to test the efficacy of using accelerometer and gyroscope sampling to
feed the entropy pool, several pieces of software had to be developed to aid in
the research. The software developed is split into two distinct applications
that work together: the java application, \textit{PollSensors} and the entropy
insertion daemon, \textit{entd}. This model was chosen because of limitations in
the security model of the Android Operating System. Being able to add entropy to
the kernel's pool is a privileged operation to prevent attackers from poisoning
the host's randomness, but Android applications can only launch as a user
process and then execute a privileged native application, such as \textit{su}.
As a result of these requirements, the entropy insertion daemon is written as a
native application that runs with privileged authority and the entropy
collection is written as an Android application that communicates with the
entropy insertion daemon using a named pipe. This model allows for working
within the confines of the security model of the Dalvik VM while performing
privileged actions securely from a privileged native application.

%%% The Testing Methodology:
%%%  - What was done
%%%  - What data was gathered
%%%  - How was it analyzed 
\subsection{Methodology}

Data collection was done using the \textit{PollSensors} and \textit{entd}
software on two Samsung Nexus S phones. Data collection was timed, and the goal
was to collect at least 10 MB worth of binary data for analysis, though due to
the speed at which certain devices operated this was not always possible. The
purpose of timing the tests was to be able to calculate the average bitrate of
the various randomness sources.

Data was collected from the kernel's stock \textit{random} and \textit{urandom}
devices, raw data from the gyroscope and accelerometer, bitfiltered versions of
that data, von Neumann whitened versions of that data, as well as from the
\textit{random} device after raw and von Neumann whitened data had been passed
into the kernel's entropy pool. Once the data had been collected, the NIST
statistical analysis software was used to determine if it had the statistical
qualities of random data or not.

